From 0ad27e03790a293d9fee7dda222a4419d184767d Mon Sep 17 00:00:00 2001
From: David Daney <david.daney@cavium.com>
Date: Mon, 20 May 2019 17:14:44 +0200
Subject: [PATCH 229/345] netdev: octeon_mgmt: Update with latest changes.

Signed-off-by: David Daney <david.daney@cavium.com>
Signed-off-by: Chandrakala Chavva <cchavva@caviumnetworks.com>
Signed-off-by: Carlos Munoz <cmunoz@caviumnetworks.com>
---
 drivers/net/ethernet/cavium/Kconfig           |   1 +
 .../net/ethernet/cavium/octeon/octeon_mgmt.c  | 809 +++++++++++-------
 2 files changed, 517 insertions(+), 293 deletions(-)

diff --git a/drivers/net/ethernet/cavium/Kconfig b/drivers/net/ethernet/cavium/Kconfig
index bc7a5bf004b2..d5bafc6f4218 100644
--- a/drivers/net/ethernet/cavium/Kconfig
+++ b/drivers/net/ethernet/cavium/Kconfig
@@ -72,6 +72,7 @@ config OCTEON_MGMT_ETHERNET
 	select PHYLIB
 	select MDIO_OCTEON
 	select OCTEON_ETHERNET_COMMON
+	select OCTEON_BGX_PORT
 	default y
 	help
 	  Enable the ethernet driver for the management
diff --git a/drivers/net/ethernet/cavium/octeon/octeon_mgmt.c b/drivers/net/ethernet/cavium/octeon/octeon_mgmt.c
index aed77b21134c..1b64be76b968 100644
--- a/drivers/net/ethernet/cavium/octeon/octeon_mgmt.c
+++ b/drivers/net/ethernet/cavium/octeon/octeon_mgmt.c
@@ -8,13 +8,11 @@
 
 #include <linux/platform_device.h>
 #include <linux/dma-mapping.h>
-#include <linux/etherdevice.h>
 #include <linux/capability.h>
 #include <linux/net_tstamp.h>
 #include <linux/interrupt.h>
 #include <linux/netdevice.h>
 #include <linux/spinlock.h>
-#include <linux/if_vlan.h>
 #include <linux/of_mdio.h>
 #include <linux/module.h>
 #include <linux/of_net.h>
@@ -23,10 +21,16 @@
 #include <linux/phy.h>
 #include <linux/io.h>
 
+#include <asm/bitfield.h>
+
 #include <asm/octeon/octeon.h>
 #include <asm/octeon/cvmx-mixx-defs.h>
 #include <asm/octeon/cvmx-agl-defs.h>
 
+#include "octeon-bgx.h"
+#include "octeon_common.h"
+#include "octeon-common-nexus.h"
+
 #define DRV_NAME "octeon_mgmt"
 #define DRV_VERSION "2.0"
 #define DRV_DESCRIPTION \
@@ -40,33 +44,16 @@
 #define OCTEON_MGMT_RX_RING_SIZE 512
 #define OCTEON_MGMT_TX_RING_SIZE 128
 
-/* Allow 8 bytes for vlan and FCS. */
-#define OCTEON_MGMT_RX_HEADROOM (ETH_HLEN + ETH_FCS_LEN + VLAN_HLEN)
+/* 14 bits of length */
+#define OCTEON_MGMT_RE_LEN_MASK 0x3fffULL
+
+/* 6 or 8 bits of code. 7th and 8th bits are always zero, so just look
+ * at 6 bits.
+ */
+#define OCTEON_MGMT_RE_CODE_MASK 0x3fULL
 
-union mgmt_port_ring_entry {
-	u64 d64;
-	struct {
 #define RING_ENTRY_CODE_DONE 0xf
 #define RING_ENTRY_CODE_MORE 0x10
-#ifdef __BIG_ENDIAN_BITFIELD
-		u64 reserved_62_63:2;
-		/* Length of the buffer/packet in bytes */
-		u64 len:14;
-		/* For TX, signals that the packet should be timestamped */
-		u64 tstamp:1;
-		/* The RX error code */
-		u64 code:7;
-		/* Physical address of the buffer */
-		u64 addr:40;
-#else
-		u64 addr:40;
-		u64 code:7;
-		u64 tstamp:1;
-		u64 len:14;
-		u64 reserved_62_63:2;
-#endif
-	} s;
-};
 
 #define MIX_ORING1	0x0
 #define MIX_ORING2	0x8
@@ -84,23 +71,12 @@ union mgmt_port_ring_entry {
 
 #define AGL_GMX_PRT_CFG			0x10
 #define AGL_GMX_RX_FRM_CTL		0x18
-#define AGL_GMX_RX_FRM_MAX		0x30
-#define AGL_GMX_RX_JABBER		0x38
 #define AGL_GMX_RX_STATS_CTL		0x50
 
 #define AGL_GMX_RX_STATS_PKTS_DRP	0xb0
 #define AGL_GMX_RX_STATS_OCTS_DRP	0xb8
 #define AGL_GMX_RX_STATS_PKTS_BAD	0xc0
 
-#define AGL_GMX_RX_ADR_CTL		0x100
-#define AGL_GMX_RX_ADR_CAM_EN		0x108
-#define AGL_GMX_RX_ADR_CAM0		0x180
-#define AGL_GMX_RX_ADR_CAM1		0x188
-#define AGL_GMX_RX_ADR_CAM2		0x190
-#define AGL_GMX_RX_ADR_CAM3		0x198
-#define AGL_GMX_RX_ADR_CAM4		0x1a0
-#define AGL_GMX_RX_ADR_CAM5		0x1a8
-
 #define AGL_GMX_TX_CLK			0x208
 #define AGL_GMX_TX_STATS_CTL		0x268
 #define AGL_GMX_TX_CTL			0x270
@@ -116,13 +92,21 @@ union mgmt_port_ring_entry {
 #define AGL_GMX_TX_STAT9		0x2c8
 
 struct octeon_mgmt {
+	/* bgx_priv Must be first element.  May be unused in the case
+	 * where the MAC is not BGX
+	 */
+	struct bgx_port_netdev_priv bgx_priv;
 	struct net_device *netdev;
 	u64 mix;
 	u64 agl;
 	u64 agl_prt_ctl;
 	int port;
+	int numa_node;
 	int irq;
+	int irq_orthresh;
+	int irq_irthresh;
 	bool has_rx_tstamp;
+	bool has_o3_structs;
 	u64 *tx_ring;
 	dma_addr_t tx_ring_handle;
 	unsigned int tx_next;
@@ -134,6 +118,10 @@ struct octeon_mgmt {
 	/* RX variables only touched in napi_poll.  No locking necessary. */
 	u64 *rx_ring;
 	dma_addr_t rx_ring_handle;
+	u64 re_addr_mask;
+	u8 re_len_shift;
+	u8 re_code_shift;
+	u8 re_ts_bit;
 	unsigned int rx_next;
 	unsigned int rx_next_fill;
 	unsigned int rx_current_fill;
@@ -181,22 +169,34 @@ static void octeon_mgmt_set_tx_irq(struct octeon_mgmt *p, int enable)
 
 static void octeon_mgmt_enable_rx_irq(struct octeon_mgmt *p)
 {
-	octeon_mgmt_set_rx_irq(p, 1);
+	if (p->has_o3_structs)
+		enable_irq(p->irq_irthresh);
+	else
+		octeon_mgmt_set_rx_irq(p, 1);
 }
 
 static void octeon_mgmt_disable_rx_irq(struct octeon_mgmt *p)
 {
-	octeon_mgmt_set_rx_irq(p, 0);
+	if (p->has_o3_structs)
+		disable_irq_nosync(p->irq_irthresh);
+	else
+		octeon_mgmt_set_rx_irq(p, 0);
 }
 
 static void octeon_mgmt_enable_tx_irq(struct octeon_mgmt *p)
 {
-	octeon_mgmt_set_tx_irq(p, 1);
+	if (p->has_o3_structs)
+		enable_irq(p->irq_orthresh);
+	else
+		octeon_mgmt_set_tx_irq(p, 1);
 }
 
 static void octeon_mgmt_disable_tx_irq(struct octeon_mgmt *p)
 {
-	octeon_mgmt_set_tx_irq(p, 0);
+	if (p->has_o3_structs)
+		disable_irq_nosync(p->irq_orthresh);
+	else
+		octeon_mgmt_set_tx_irq(p, 0);
 }
 
 static unsigned int ring_max_fill(unsigned int ring_size)
@@ -206,7 +206,7 @@ static unsigned int ring_max_fill(unsigned int ring_size)
 
 static unsigned int ring_size_to_bytes(unsigned int ring_size)
 {
-	return ring_size * sizeof(union mgmt_port_ring_entry);
+	return ring_size * sizeof(u64);
 }
 
 static void octeon_mgmt_rx_fill_ring(struct net_device *netdev)
@@ -214,12 +214,12 @@ static void octeon_mgmt_rx_fill_ring(struct net_device *netdev)
 	struct octeon_mgmt *p = netdev_priv(netdev);
 
 	while (p->rx_current_fill < ring_max_fill(OCTEON_MGMT_RX_RING_SIZE)) {
-		unsigned int size;
-		union mgmt_port_ring_entry re;
+		u64 size;
+		u64 raw_re;
 		struct sk_buff *skb;
 
 		/* CN56XX pass 1 needs 8 bytes of padding.  */
-		size = netdev->mtu + OCTEON_MGMT_RX_HEADROOM + 8 + NET_IP_ALIGN;
+		size = netdev->mtu + OCTEON_FRAME_HEADER_LEN + 8 + NET_IP_ALIGN;
 
 		skb = netdev_alloc_skb(netdev, size);
 		if (!skb)
@@ -227,14 +227,13 @@ static void octeon_mgmt_rx_fill_ring(struct net_device *netdev)
 		skb_reserve(skb, NET_IP_ALIGN);
 		__skb_queue_tail(&p->rx_list, skb);
 
-		re.d64 = 0;
-		re.s.len = size;
-		re.s.addr = dma_map_single(p->dev, skb->data,
-					   size,
-					   DMA_FROM_DEVICE);
+		raw_re = dma_map_single(p->dev, skb->data,
+					size,
+					DMA_FROM_DEVICE);
+		raw_re |= (size << p->re_len_shift);
 
 		/* Put it in the ring.  */
-		p->rx_ring[p->rx_next_fill] = re.d64;
+		p->rx_ring[p->rx_next_fill] = raw_re;
 		dma_sync_single_for_device(p->dev, p->rx_ring_handle,
 					   ring_size_to_bytes(OCTEON_MGMT_RX_RING_SIZE),
 					   DMA_BIDIRECTIONAL);
@@ -249,12 +248,13 @@ static void octeon_mgmt_rx_fill_ring(struct net_device *netdev)
 static void octeon_mgmt_clean_tx_buffers(struct octeon_mgmt *p)
 {
 	union cvmx_mixx_orcnt mix_orcnt;
-	union mgmt_port_ring_entry re;
+	u64 re;
 	struct sk_buff *skb;
 	int cleaned = 0;
 	unsigned long flags;
 
 	mix_orcnt.u64 = cvmx_read_csr(p->mix + MIX_ORCNT);
+
 	while (mix_orcnt.s.orcnt) {
 		spin_lock_irqsave(&p->tx_list.lock, flags);
 
@@ -269,7 +269,7 @@ static void octeon_mgmt_clean_tx_buffers(struct octeon_mgmt *p)
 					ring_size_to_bytes(OCTEON_MGMT_TX_RING_SIZE),
 					DMA_BIDIRECTIONAL);
 
-		re.d64 = p->tx_ring[p->tx_next_clean];
+		re = p->tx_ring[p->tx_next_clean];
 		p->tx_next_clean =
 			(p->tx_next_clean + 1) % OCTEON_MGMT_TX_RING_SIZE;
 		skb = __skb_dequeue(&p->tx_list);
@@ -283,11 +283,12 @@ static void octeon_mgmt_clean_tx_buffers(struct octeon_mgmt *p)
 
 		spin_unlock_irqrestore(&p->tx_list.lock, flags);
 
-		dma_unmap_single(p->dev, re.s.addr, re.s.len,
+		dma_unmap_single(p->dev, re & p->re_addr_mask,
+				 (re >> p->re_len_shift) & OCTEON_MGMT_RE_LEN_MASK,
 				 DMA_TO_DEVICE);
 
 		/* Read the hardware TX timestamp if one was recorded */
-		if (unlikely(re.s.tstamp)) {
+		if (unlikely((re >> p->re_ts_bit) & 1)) {
 			struct skb_shared_hwtstamps ts;
 			u64 ns;
 
@@ -306,6 +307,12 @@ static void octeon_mgmt_clean_tx_buffers(struct octeon_mgmt *p)
 
 		mix_orcnt.u64 = cvmx_read_csr(p->mix + MIX_ORCNT);
 	}
+	if (p->has_o3_structs) {
+		union cvmx_mixx_isr mixx_isr;
+		mixx_isr.u64 = 0;
+		mixx_isr.s.orthresh = 1;
+		cvmx_write_csr(p->mix + MIX_ISR, mixx_isr.u64);
+	}
 
 	if (cleaned && netif_queue_stopped(p->netdev))
 		netif_wake_queue(p->netdev);
@@ -324,6 +331,9 @@ static void octeon_mgmt_update_rx_stats(struct net_device *netdev)
 	unsigned long flags;
 	u64 drop, bad;
 
+	if (p->has_o3_structs)
+		return;
+
 	/* These reads also clear the count registers.  */
 	drop = cvmx_read_csr(p->agl + AGL_GMX_RX_STATS_PKTS_DRP);
 	bad = cvmx_read_csr(p->agl + AGL_GMX_RX_STATS_PKTS_BAD);
@@ -365,22 +375,22 @@ static void octeon_mgmt_update_tx_stats(struct net_device *netdev)
 static u64 octeon_mgmt_dequeue_rx_buffer(struct octeon_mgmt *p,
 					 struct sk_buff **pskb)
 {
-	union mgmt_port_ring_entry re;
+	u64 entry;
 
 	dma_sync_single_for_cpu(p->dev, p->rx_ring_handle,
 				ring_size_to_bytes(OCTEON_MGMT_RX_RING_SIZE),
 				DMA_BIDIRECTIONAL);
 
-	re.d64 = p->rx_ring[p->rx_next];
+	entry = p->rx_ring[p->rx_next];
 	p->rx_next = (p->rx_next + 1) % OCTEON_MGMT_RX_RING_SIZE;
 	p->rx_current_fill--;
 	*pskb = __skb_dequeue(&p->rx_list);
 
-	dma_unmap_single(p->dev, re.s.addr,
-			 ETH_FRAME_LEN + OCTEON_MGMT_RX_HEADROOM,
+	dma_unmap_single(p->dev, entry & p->re_addr_mask,
+			 ETH_FRAME_LEN + OCTEON_FRAME_HEADER_LEN,
 			 DMA_FROM_DEVICE);
 
-	return re.d64;
+	return entry;
 }
 
 
@@ -388,18 +398,22 @@ static int octeon_mgmt_receive_one(struct octeon_mgmt *p)
 {
 	struct net_device *netdev = p->netdev;
 	union cvmx_mixx_ircnt mix_ircnt;
-	union mgmt_port_ring_entry re;
+	u64 re;
+	int code, code2;
+	int len, len2;
 	struct sk_buff *skb;
 	struct sk_buff *skb2;
 	struct sk_buff *skb_new;
-	union mgmt_port_ring_entry re2;
+	u64 re2;
 	int rc = 1;
 
 
-	re.d64 = octeon_mgmt_dequeue_rx_buffer(p, &skb);
-	if (likely(re.s.code == RING_ENTRY_CODE_DONE)) {
+	re = octeon_mgmt_dequeue_rx_buffer(p, &skb);
+	code = (re >> p->re_code_shift) & OCTEON_MGMT_RE_CODE_MASK;
+	len = (re >> p->re_len_shift) & OCTEON_MGMT_RE_LEN_MASK;
+	if (likely(code == RING_ENTRY_CODE_DONE)) {
 		/* A good packet, send it up. */
-		skb_put(skb, re.s.len);
+		skb_put(skb, len);
 good:
 		/* Process the RX timestamp if it was recorded */
 		if (p->has_rx_tstamp) {
@@ -415,7 +429,7 @@ static int octeon_mgmt_receive_one(struct octeon_mgmt *p)
 		netdev->stats.rx_bytes += skb->len;
 		netif_receive_skb(skb);
 		rc = 0;
-	} else if (re.s.code == RING_ENTRY_CODE_MORE) {
+	} else if (code == RING_ENTRY_CODE_MORE) {
 		/* Packet split across skbs.  This can happen if we
 		 * increase the MTU.  Buffers that are already in the
 		 * rx ring can then end up being too small.  As the rx
@@ -423,13 +437,15 @@ static int octeon_mgmt_receive_one(struct octeon_mgmt *p)
 		 * will be used and we should go back to the normal
 		 * non-split case.
 		 */
-		skb_put(skb, re.s.len);
+		skb_put(skb, len);
 		do {
-			re2.d64 = octeon_mgmt_dequeue_rx_buffer(p, &skb2);
-			if (re2.s.code != RING_ENTRY_CODE_MORE
-				&& re2.s.code != RING_ENTRY_CODE_DONE)
+			re2 = octeon_mgmt_dequeue_rx_buffer(p, &skb2);
+			code2 = (re2 >> p->re_code_shift) & OCTEON_MGMT_RE_CODE_MASK;
+			len2 = (re2 >> p->re_len_shift) & OCTEON_MGMT_RE_LEN_MASK;
+			if (code2 != RING_ENTRY_CODE_MORE
+				&& code2 != RING_ENTRY_CODE_DONE)
 				goto split_error;
-			skb_put(skb2,  re2.s.len);
+			skb_put(skb2,  len2);
 			skb_new = skb_copy_expand(skb, 0, skb2->len,
 						  GFP_ATOMIC);
 			if (!skb_new)
@@ -441,7 +457,7 @@ static int octeon_mgmt_receive_one(struct octeon_mgmt *p)
 			dev_kfree_skb_any(skb);
 			dev_kfree_skb_any(skb2);
 			skb = skb_new;
-		} while (re2.s.code == RING_ENTRY_CODE_MORE);
+		} while (code2 == RING_ENTRY_CODE_MORE);
 		goto good;
 	} else {
 		/* Some other error, discard it. */
@@ -455,8 +471,9 @@ static int octeon_mgmt_receive_one(struct octeon_mgmt *p)
 	/* Discard the whole mess. */
 	dev_kfree_skb_any(skb);
 	dev_kfree_skb_any(skb2);
-	while (re2.s.code == RING_ENTRY_CODE_MORE) {
-		re2.d64 = octeon_mgmt_dequeue_rx_buffer(p, &skb2);
+	while (code2 == RING_ENTRY_CODE_MORE) {
+		re2 = octeon_mgmt_dequeue_rx_buffer(p, &skb2);
+		code2 = (re2 >> p->re_code_shift) & OCTEON_MGMT_RE_CODE_MASK;
 		dev_kfree_skb_any(skb2);
 	}
 	netdev->stats.rx_errors++;
@@ -502,6 +519,12 @@ static int octeon_mgmt_napi_poll(struct napi_struct *napi, int budget)
 	if (work_done < budget) {
 		/* We stopped because no more packets were available. */
 		napi_complete_done(napi, work_done);
+		if (p->has_o3_structs) {
+			union cvmx_mixx_isr mixx_isr;
+			mixx_isr.u64 = 0;
+			mixx_isr.s.irthresh = 1;
+			cvmx_write_csr(p->mix + MIX_ISR, mixx_isr.u64);
+		}
 		octeon_mgmt_enable_rx_irq(p);
 	}
 	octeon_mgmt_update_rx_stats(netdev);
@@ -510,11 +533,10 @@ static int octeon_mgmt_napi_poll(struct napi_struct *napi, int budget)
 }
 
 /* Reset the hardware to clean state.  */
-static void octeon_mgmt_reset_hw(struct octeon_mgmt *p)
+static void octeon_mgmt_common_reset_hw(struct octeon_mgmt *p)
 {
 	union cvmx_mixx_ctl mix_ctl;
 	union cvmx_mixx_bist mix_bist;
-	union cvmx_agl_gmx_bist agl_gmx_bist;
 
 	mix_ctl.u64 = 0;
 	cvmx_write_csr(p->mix + MIX_CTL, mix_ctl.u64);
@@ -531,111 +553,33 @@ static void octeon_mgmt_reset_hw(struct octeon_mgmt *p)
 		dev_warn(p->dev, "MIX failed BIST (0x%016llx)\n",
 			(unsigned long long)mix_bist.u64);
 
-	agl_gmx_bist.u64 = cvmx_read_csr(CVMX_AGL_GMX_BIST);
-	if (agl_gmx_bist.u64)
-		dev_warn(p->dev, "AGL failed BIST (0x%016llx)\n",
-			 (unsigned long long)agl_gmx_bist.u64);
 }
 
-struct octeon_mgmt_cam_state {
-	u64 cam[6];
-	u64 cam_mask;
-	int cam_index;
-};
-
-static void octeon_mgmt_cam_state_add(struct octeon_mgmt_cam_state *cs,
-				      unsigned char *addr)
+/* Reset the hardware to clean state.  */
+static void octeon_mgmt_reset_hw(struct octeon_mgmt *p)
 {
-	int i;
+	union cvmx_agl_gmx_bist agl_gmx_bist;
+
+	octeon_mgmt_common_reset_hw(p);
 
-	for (i = 0; i < 6; i++)
-		cs->cam[i] |= (u64)addr[i] << (8 * (cs->cam_index));
-	cs->cam_mask |= (1ULL << cs->cam_index);
-	cs->cam_index++;
+	agl_gmx_bist.u64 = cvmx_read_csr(CVMX_AGL_GMX_BIST);
+	if (agl_gmx_bist.u64)
+		dev_warn(p->dev, "AGL failed BIST (0x%016llx)\n",
+			 (unsigned long long)agl_gmx_bist.u64);
 }
 
 static void octeon_mgmt_set_rx_filtering(struct net_device *netdev)
 {
 	struct octeon_mgmt *p = netdev_priv(netdev);
-	union cvmx_agl_gmx_rxx_adr_ctl adr_ctl;
-	union cvmx_agl_gmx_prtx_cfg agl_gmx_prtx;
-	unsigned long flags;
-	unsigned int prev_packet_enable;
-	unsigned int cam_mode = 1; /* 1 - Accept on CAM match */
-	unsigned int multicast_mode = 1; /* 1 - Reject all multicast.  */
-	struct octeon_mgmt_cam_state cam_state;
-	struct netdev_hw_addr *ha;
-	int available_cam_entries;
-
-	memset(&cam_state, 0, sizeof(cam_state));
-
-	if ((netdev->flags & IFF_PROMISC) || netdev->uc.count > 7) {
-		cam_mode = 0;
-		available_cam_entries = 8;
-	} else {
-		/* One CAM entry for the primary address, leaves seven
-		 * for the secondary addresses.
-		 */
-		available_cam_entries = 7 - netdev->uc.count;
-	}
 
-	if (netdev->flags & IFF_MULTICAST) {
-		if (cam_mode == 0 || (netdev->flags & IFF_ALLMULTI) ||
-		    netdev_mc_count(netdev) > available_cam_entries)
-			multicast_mode = 2; /* 2 - Accept all multicast.  */
-		else
-			multicast_mode = 0; /* 0 - Use CAM.  */
-	}
-
-	if (cam_mode == 1) {
-		/* Add primary address. */
-		octeon_mgmt_cam_state_add(&cam_state, netdev->dev_addr);
-		netdev_for_each_uc_addr(ha, netdev)
-			octeon_mgmt_cam_state_add(&cam_state, ha->addr);
-	}
-	if (multicast_mode == 0) {
-		netdev_for_each_mc_addr(ha, netdev)
-			octeon_mgmt_cam_state_add(&cam_state, ha->addr);
-	}
-
-	spin_lock_irqsave(&p->lock, flags);
-
-	/* Disable packet I/O. */
-	agl_gmx_prtx.u64 = cvmx_read_csr(p->agl + AGL_GMX_PRT_CFG);
-	prev_packet_enable = agl_gmx_prtx.s.en;
-	agl_gmx_prtx.s.en = 0;
-	cvmx_write_csr(p->agl + AGL_GMX_PRT_CFG, agl_gmx_prtx.u64);
-
-	adr_ctl.u64 = 0;
-	adr_ctl.s.cam_mode = cam_mode;
-	adr_ctl.s.mcst = multicast_mode;
-	adr_ctl.s.bcst = 1;     /* Allow broadcast */
-
-	cvmx_write_csr(p->agl + AGL_GMX_RX_ADR_CTL, adr_ctl.u64);
-
-	cvmx_write_csr(p->agl + AGL_GMX_RX_ADR_CAM0, cam_state.cam[0]);
-	cvmx_write_csr(p->agl + AGL_GMX_RX_ADR_CAM1, cam_state.cam[1]);
-	cvmx_write_csr(p->agl + AGL_GMX_RX_ADR_CAM2, cam_state.cam[2]);
-	cvmx_write_csr(p->agl + AGL_GMX_RX_ADR_CAM3, cam_state.cam[3]);
-	cvmx_write_csr(p->agl + AGL_GMX_RX_ADR_CAM4, cam_state.cam[4]);
-	cvmx_write_csr(p->agl + AGL_GMX_RX_ADR_CAM5, cam_state.cam[5]);
-	cvmx_write_csr(p->agl + AGL_GMX_RX_ADR_CAM_EN, cam_state.cam_mask);
-
-	/* Restore packet I/O. */
-	agl_gmx_prtx.s.en = prev_packet_enable;
-	cvmx_write_csr(p->agl + AGL_GMX_PRT_CFG, agl_gmx_prtx.u64);
-
-	spin_unlock_irqrestore(&p->lock, flags);
+	cvm_oct_common_set_rx_filtering(netdev, p->agl, &p->lock);
 }
 
 static int octeon_mgmt_set_mac_address(struct net_device *netdev, void *addr)
 {
-	int r = eth_mac_addr(netdev, addr);
-
-	if (r)
-		return r;
+	struct octeon_mgmt *p = netdev_priv(netdev);
 
-	octeon_mgmt_set_rx_filtering(netdev);
+	cvm_oct_common_set_mac_address(netdev, addr, p->agl, &p->lock);
 
 	return 0;
 }
@@ -643,21 +587,14 @@ static int octeon_mgmt_set_mac_address(struct net_device *netdev, void *addr)
 static int octeon_mgmt_change_mtu(struct net_device *netdev, int new_mtu)
 {
 	struct octeon_mgmt *p = netdev_priv(netdev);
-	int max_packet = new_mtu + ETH_HLEN + ETH_FCS_LEN;
-
-	netdev->mtu = new_mtu;
 
-	/* HW lifts the limit if the frame is VLAN tagged
-	 * (+4 bytes per each tag, up to two tags)
-	 */
-	cvmx_write_csr(p->agl + AGL_GMX_RX_FRM_MAX, max_packet);
-	/* Set the hardware to truncate packets larger than the MTU. The jabber
-	 * register must be set to a multiple of 8 bytes, so round up. JABBER is
-	 * an unconditional limit, so we need to account for two possible VLAN
-	 * tags.
+	/* Limit the MTU to make sure the ethernet packets are between
+	 * 64 bytes and 16383 bytes.
 	 */
-	cvmx_write_csr(p->agl + AGL_GMX_RX_JABBER,
-		       (max_packet + 7 + VLAN_HLEN * 2) & 0xfff8);
+	int ret = cvm_oct_common_change_mtu(netdev, new_mtu, p->agl, 0, 16383);
+
+	if (ret)
+		return ret;
 
 	return 0;
 }
@@ -686,6 +623,26 @@ static irqreturn_t octeon_mgmt_interrupt(int cpl, void *dev_id)
 	return IRQ_HANDLED;
 }
 
+static irqreturn_t octeon_mgmt_o3_or_handler(int irq, void *dev_id)
+{
+	struct octeon_mgmt *p = dev_id;
+
+	octeon_mgmt_disable_tx_irq(p);
+	tasklet_schedule(&p->tx_clean_tasklet);
+
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t octeon_mgmt_o3_ir_handler(int irq, void *dev_id)
+{
+	struct octeon_mgmt *p = dev_id;
+
+	octeon_mgmt_disable_rx_irq(p);
+	napi_schedule(&p->napi);
+
+	return IRQ_HANDLED;
+}
+
 static int octeon_mgmt_ioctl_hwtstamp(struct net_device *netdev,
 				      struct ifreq *rq, int cmd)
 {
@@ -795,6 +752,14 @@ static int octeon_mgmt_ioctl(struct net_device *netdev,
 	}
 }
 
+static int octeon_mgmt_o3_ioctl(struct net_device *netdev,
+				struct ifreq *rq, int cmd)
+{
+	if (netdev->phydev)
+		return phy_mii_ioctl(netdev->phydev, rq, cmd);
+	return -EINVAL;
+}
+
 static void octeon_mgmt_disable_link(struct octeon_mgmt *p)
 {
 	union cvmx_agl_gmx_prtx_cfg prtx_cfg;
@@ -838,7 +803,10 @@ static void octeon_mgmt_update_link(struct octeon_mgmt *p)
 
 	prtx_cfg.u64 = cvmx_read_csr(p->agl + AGL_GMX_PRT_CFG);
 
-	if (!phydev->link)
+	if (!ndev->phydev)
+		return;
+
+	if (!ndev->phydev->link)
 		prtx_cfg.s.duplex = 1;
 	else
 		prtx_cfg.s.duplex = phydev->duplex;
@@ -966,18 +934,70 @@ static int octeon_mgmt_init_phy(struct net_device *netdev)
 	return 0;
 }
 
-static int octeon_mgmt_open(struct net_device *netdev)
+static int octeon_mgmt_release_mem(struct octeon_mgmt *p)
+{
+	/* dma_unmap is a nop on Octeon, so just free everything.  */
+	skb_queue_purge(&p->tx_list);
+	skb_queue_purge(&p->rx_list);
+
+	if (p->rx_ring_handle)
+		dma_unmap_single(p->dev, p->rx_ring_handle,
+				 ring_size_to_bytes(OCTEON_MGMT_RX_RING_SIZE),
+				 DMA_BIDIRECTIONAL);
+	kfree(p->rx_ring);
+
+	if (p->tx_ring_handle)
+		dma_unmap_single(p->dev, p->tx_ring_handle,
+				 ring_size_to_bytes(OCTEON_MGMT_TX_RING_SIZE),
+				 DMA_BIDIRECTIONAL);
+	kfree(p->tx_ring);
+
+	return 0;
+}
+
+static int octeon_mgmt_stop(struct net_device *netdev)
+{
+	struct octeon_mgmt *p = netdev_priv(netdev);
+
+	napi_disable(&p->napi);
+	netif_stop_queue(netdev);
+
+	if (netdev->phydev)
+		phy_disconnect(netdev->phydev);
+
+	netif_carrier_off(netdev);
+
+	octeon_mgmt_reset_hw(p);
+
+	if (p->irq)
+		free_irq(p->irq, netdev);
+
+	return octeon_mgmt_release_mem(p);
+}
+
+static int octeon_mgmt_o3_stop(struct net_device *netdev)
+{
+	struct octeon_mgmt *p = netdev_priv(netdev);
+
+	napi_disable(&p->napi);
+	netif_stop_queue(netdev);
+
+	bgx_port_disable(netdev);
+
+	octeon_mgmt_reset_hw(p);
+
+
+	if (p->irq_orthresh)
+		free_irq(p->irq_orthresh, p);
+	if (p->irq_irthresh)
+		free_irq(p->irq_irthresh, p);
+
+	return octeon_mgmt_release_mem(p);
+}
+
+static int octeon_mgmt_alloc_rings(struct net_device *netdev)
 {
 	struct octeon_mgmt *p = netdev_priv(netdev);
-	union cvmx_mixx_ctl mix_ctl;
-	union cvmx_agl_gmx_inf_mode agl_gmx_inf_mode;
-	union cvmx_mixx_oring1 oring1;
-	union cvmx_mixx_iring1 iring1;
-	union cvmx_agl_gmx_rxx_frm_ctl rxx_frm_ctl;
-	union cvmx_mixx_irhwm mix_irhwm;
-	union cvmx_mixx_orhwm mix_orhwm;
-	union cvmx_mixx_intena mix_intena;
-	struct sockaddr sa;
 
 	/* Allocate ring buffers.  */
 	p->tx_ring = kzalloc(ring_size_to_bytes(OCTEON_MGMT_TX_RING_SIZE),
@@ -996,7 +1016,7 @@ static int octeon_mgmt_open(struct net_device *netdev)
 	p->rx_ring = kzalloc(ring_size_to_bytes(OCTEON_MGMT_RX_RING_SIZE),
 			     GFP_KERNEL);
 	if (!p->rx_ring)
-		goto err_nomem;
+		return -ENOMEM;
 	p->rx_ring_handle =
 		dma_map_single(p->dev, p->rx_ring,
 			       ring_size_to_bytes(OCTEON_MGMT_RX_RING_SIZE),
@@ -1006,6 +1026,125 @@ static int octeon_mgmt_open(struct net_device *netdev)
 	p->rx_next_fill = 0;
 	p->rx_current_fill = 0;
 
+	return 0;
+}
+
+static int octeon_mgmt_o3_open(struct net_device *netdev)
+{
+	struct octeon_mgmt *p = netdev_priv(netdev);
+	union cvmx_mixx_ctl mix_ctl;
+	union cvmx_mixx_oring1 oring1;
+	union cvmx_mixx_iring1 iring1;
+	union cvmx_mixx_irhwm mix_irhwm;
+	union cvmx_mixx_orhwm mix_orhwm;
+	struct irq_domain *domain;
+	int intsn_orthresh;
+	int intsn_irthresh;
+
+	if (octeon_mgmt_alloc_rings(netdev))
+		goto err_nomem;
+
+	bgx_port_mix_assert_reset(netdev, p->port, true);
+	octeon_mgmt_common_reset_hw(p);
+
+	mix_ctl.u64 = cvmx_read_csr(p->mix + MIX_CTL);
+
+	/* Bring it out of reset if needed. */
+	if (mix_ctl.s.reset) {
+		mix_ctl.s.reset = 0;
+		cvmx_write_csr(p->mix + MIX_CTL, mix_ctl.u64);
+		do {
+			mix_ctl.u64 = cvmx_read_csr(p->mix + MIX_CTL);
+		} while (mix_ctl.s.reset);
+	}
+	bgx_port_mix_assert_reset(netdev, p->port, false);
+
+	oring1.u64 = 0;
+	oring1.cn78xx.obase = p->tx_ring_handle >> 3;
+	oring1.cn78xx.osize = OCTEON_MGMT_TX_RING_SIZE;
+	cvmx_write_csr(p->mix + MIX_ORING1, oring1.u64);
+
+	iring1.u64 = 0;
+	iring1.cn78xx.ibase = p->rx_ring_handle >> 3;
+	iring1.cn78xx.isize = OCTEON_MGMT_RX_RING_SIZE;
+	cvmx_write_csr(p->mix + MIX_IRING1, iring1.u64);
+
+	/* Enable the port HW. Packets are not allowed until
+	 * cvmx_mgmt_port_enable() is called.
+	 */
+	mix_ctl.u64 = 0;
+	mix_ctl.s.crc_strip = 1;    /* Strip the ending CRC */
+	mix_ctl.s.en = 1;           /* Enable the port */
+	mix_ctl.s.nbtarb = 0;       /* Arbitration mode */
+	/* MII CB-request FIFO programmable high watermark */
+	mix_ctl.s.mrq_hwm = 1;
+#ifdef __LITTLE_ENDIAN
+	mix_ctl.s.lendian = 1;
+#endif
+	cvmx_write_csr(p->mix + MIX_CTL, mix_ctl.u64);
+
+	octeon_mgmt_rx_fill_ring(netdev);
+
+	/* Clear any pending interrupts */
+	cvmx_write_csr(p->mix + MIX_ISR, cvmx_read_csr(p->mix + MIX_ISR));
+
+	/* Interrupt every single RX packet */
+	mix_irhwm.u64 = 0;
+	mix_irhwm.s.irhwm = 0;
+	cvmx_write_csr(p->mix + MIX_IRHWM, mix_irhwm.u64);
+
+	/* Interrupt when we have 1 or more packets to clean.  */
+	mix_orhwm.u64 = 0;
+	mix_orhwm.s.orhwm = 0;
+	cvmx_write_csr(p->mix + MIX_ORHWM, mix_orhwm.u64);
+
+	napi_enable(&p->napi);
+
+	intsn_orthresh = 0x10002 + 0x10 * p->port;
+	intsn_irthresh = 0x10003 + 0x10 * p->port;
+
+	domain = octeon_irq_get_block_domain(p->numa_node, intsn_orthresh >> 12);
+	p->irq_orthresh = irq_create_mapping(domain, intsn_orthresh);
+	p->irq_irthresh = irq_create_mapping(domain, intsn_irthresh);
+
+	if (request_irq(p->irq_orthresh, octeon_mgmt_o3_or_handler, 0, netdev->name, p)) {
+		netdev_err(netdev, "request_irq(%d) failed.\n", p->irq_orthresh);
+		goto err_noirq;
+	}
+	if (request_irq(p->irq_irthresh, octeon_mgmt_o3_ir_handler, 0, netdev->name, p)) {
+		netdev_err(netdev, "request_irq(%d) failed.\n", p->irq_irthresh);
+		goto err_noirq;
+	}
+
+	p->last_link = 0;
+	p->last_speed = 0;
+
+	netif_wake_queue(netdev);
+
+	return bgx_port_enable(netdev);
+
+err_noirq:
+err_nomem:
+	octeon_mgmt_o3_stop(netdev);
+	return -ENOMEM;
+}
+
+static int octeon_mgmt_open(struct net_device *netdev)
+{
+	struct octeon_mgmt *p = netdev_priv(netdev);
+	union cvmx_mixx_ctl mix_ctl;
+	union cvmx_agl_gmx_inf_mode agl_gmx_inf_mode;
+	union cvmx_mixx_oring1 oring1;
+	union cvmx_mixx_iring1 iring1;
+	union cvmx_agl_gmx_rxx_frm_ctl rxx_frm_ctl;
+	union cvmx_mixx_irhwm mix_irhwm;
+	union cvmx_mixx_orhwm mix_orhwm;
+	union cvmx_mixx_intena mix_intena;
+	struct sockaddr sa;
+
+	if (octeon_mgmt_alloc_rings(netdev))
+		goto err_nomem;
+
 	octeon_mgmt_reset_hw(p);
 
 	mix_ctl.u64 = cvmx_read_csr(p->mix + MIX_CTL);
@@ -1168,6 +1307,8 @@ static int octeon_mgmt_open(struct net_device *netdev)
 	mix_orhwm.s.orhwm = 0;
 	cvmx_write_csr(p->mix + MIX_ORHWM, mix_orhwm.u64);
 
+	napi_enable(&p->napi);
+
 	/* Enable receive and transmit interrupts */
 	mix_intena.u64 = 0;
 	mix_intena.s.ithena = 1;
@@ -1220,69 +1361,30 @@ static int octeon_mgmt_open(struct net_device *netdev)
 	}
 
 	netif_wake_queue(netdev);
-	napi_enable(&p->napi);
 
 	return 0;
+
 err_noirq:
-	octeon_mgmt_reset_hw(p);
-	dma_unmap_single(p->dev, p->rx_ring_handle,
-			 ring_size_to_bytes(OCTEON_MGMT_RX_RING_SIZE),
-			 DMA_BIDIRECTIONAL);
-	kfree(p->rx_ring);
 err_nomem:
-	dma_unmap_single(p->dev, p->tx_ring_handle,
-			 ring_size_to_bytes(OCTEON_MGMT_TX_RING_SIZE),
-			 DMA_BIDIRECTIONAL);
-	kfree(p->tx_ring);
+	octeon_mgmt_stop(netdev);
 	return -ENOMEM;
 }
 
-static int octeon_mgmt_stop(struct net_device *netdev)
-{
-	struct octeon_mgmt *p = netdev_priv(netdev);
-
-	napi_disable(&p->napi);
-	netif_stop_queue(netdev);
-
-	if (netdev->phydev)
-		phy_disconnect(netdev->phydev);
-
-	netif_carrier_off(netdev);
-
-	octeon_mgmt_reset_hw(p);
-
-	free_irq(p->irq, netdev);
-
-	/* dma_unmap is a nop on Octeon, so just free everything.  */
-	skb_queue_purge(&p->tx_list);
-	skb_queue_purge(&p->rx_list);
-
-	dma_unmap_single(p->dev, p->rx_ring_handle,
-			 ring_size_to_bytes(OCTEON_MGMT_RX_RING_SIZE),
-			 DMA_BIDIRECTIONAL);
-	kfree(p->rx_ring);
-
-	dma_unmap_single(p->dev, p->tx_ring_handle,
-			 ring_size_to_bytes(OCTEON_MGMT_TX_RING_SIZE),
-			 DMA_BIDIRECTIONAL);
-	kfree(p->tx_ring);
-
-	return 0;
-}
-
 static int octeon_mgmt_xmit(struct sk_buff *skb, struct net_device *netdev)
 {
 	struct octeon_mgmt *p = netdev_priv(netdev);
-	union mgmt_port_ring_entry re;
 	unsigned long flags;
 	int rv = NETDEV_TX_BUSY;
+	u64 re;
+	u64 addr;
+
+	addr = dma_map_single(p->dev, skb->data,
+			      skb->len,
+			      DMA_TO_DEVICE);
 
-	re.d64 = 0;
-	re.s.tstamp = ((skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) != 0);
-	re.s.len = skb->len;
-	re.s.addr = dma_map_single(p->dev, skb->data,
-				   skb->len,
-				   DMA_TO_DEVICE);
+	re = addr | (((u64)skb->len & OCTEON_MGMT_RE_LEN_MASK) << p->re_len_shift);
+	if ((skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP))
+		re |= 1ull << p->re_ts_bit;
 
 	spin_lock_irqsave(&p->tx_list.lock, flags);
 
@@ -1295,7 +1397,7 @@ static int octeon_mgmt_xmit(struct sk_buff *skb, struct net_device *netdev)
 	if (unlikely(p->tx_current_fill >=
 		     ring_max_fill(OCTEON_MGMT_TX_RING_SIZE))) {
 		spin_unlock_irqrestore(&p->tx_list.lock, flags);
-		dma_unmap_single(p->dev, re.s.addr, re.s.len,
+		dma_unmap_single(p->dev, addr, skb->len,
 				 DMA_TO_DEVICE);
 		goto out;
 	}
@@ -1303,7 +1405,7 @@ static int octeon_mgmt_xmit(struct sk_buff *skb, struct net_device *netdev)
 	__skb_queue_tail(&p->tx_list, skb);
 
 	/* Put it in the ring.  */
-	p->tx_ring[p->tx_next] = re.d64;
+	p->tx_ring[p->tx_next] = re;
 	p->tx_next = (p->tx_next + 1) % OCTEON_MGMT_TX_RING_SIZE;
 	p->tx_current_fill++;
 
@@ -1322,7 +1424,8 @@ static int octeon_mgmt_xmit(struct sk_buff *skb, struct net_device *netdev)
 	netif_trans_update(netdev);
 	rv = NETDEV_TX_OK;
 out:
-	octeon_mgmt_update_tx_stats(netdev);
+	if (!p->has_o3_structs)
+		octeon_mgmt_update_tx_stats(netdev);
 	return rv;
 }
 
@@ -1336,6 +1439,34 @@ static void octeon_mgmt_poll_controller(struct net_device *netdev)
 }
 #endif
 
+static int octeon_mgmt_o3_init(struct net_device *netdev)
+{
+	const u8 *mac;
+
+	mac = bgx_port_get_mac(netdev);
+	if (mac && is_valid_ether_addr(mac)) {
+		memcpy(netdev->dev_addr, mac, ETH_ALEN);
+		netdev->addr_assign_type &= ~NET_ADDR_RANDOM;
+	} else {
+		eth_hw_addr_random(netdev);
+	}
+	bgx_port_set_rx_filtering(netdev);
+
+	return bgx_port_change_mtu(netdev, netdev->mtu);
+}
+
+static int octeon_mgmt_o3_set_mac_address(struct net_device *netdev, void *addr)
+{
+	int r = eth_mac_addr(netdev, addr);
+
+	if (r)
+		return r;
+
+	bgx_port_set_rx_filtering(netdev);
+
+	return 0;
+}
+
 static void octeon_mgmt_get_drvinfo(struct net_device *netdev,
 				    struct ethtool_drvinfo *info)
 {
@@ -1377,6 +1508,125 @@ static const struct net_device_ops octeon_mgmt_ops = {
 #endif
 };
 
+static const struct ethtool_ops octeon_mgmt_o3_ethtool_ops = {
+	.get_drvinfo = octeon_mgmt_get_drvinfo,
+	.get_settings = bgx_port_ethtool_get_settings,
+	.set_settings = bgx_port_ethtool_set_settings,
+	.nway_reset = bgx_port_ethtool_nway_reset,
+	.get_link = ethtool_op_get_link,
+};
+
+static const struct net_device_ops octeon_mgmt_o3_ops = {
+	.ndo_init		= octeon_mgmt_o3_init,
+	.ndo_open		= octeon_mgmt_o3_open,
+	.ndo_stop		= octeon_mgmt_o3_stop,
+	.ndo_start_xmit		= octeon_mgmt_xmit,
+	.ndo_set_rx_mode	= bgx_port_set_rx_filtering,
+	.ndo_set_mac_address	= octeon_mgmt_o3_set_mac_address,
+	.ndo_do_ioctl		= octeon_mgmt_o3_ioctl,
+	.ndo_change_mtu		= bgx_port_change_mtu,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller	= octeon_mgmt_poll_controller,
+#endif
+};
+
+static int octeon_mgmt_remove(struct platform_device *pdev)
+{
+	struct net_device *netdev = dev_get_drvdata(&pdev->dev);
+	struct octeon_mgmt *p = netdev_priv(netdev);
+
+	if (p->napi.dev)
+		netif_napi_del(&p->napi);
+	tasklet_kill(&p->tx_clean_tasklet);
+	unregister_netdev(netdev);
+	dev_set_drvdata(&pdev->dev, NULL);
+	free_netdev(netdev);
+
+	return 0;
+}
+
+static void octeon_mgmt_probe_common(struct net_device *netdev,
+				     struct platform_device *pdev)
+{
+	struct octeon_mgmt *p = netdev_priv(netdev);
+
+	SET_NETDEV_DEV(netdev, &pdev->dev);
+	dev_set_drvdata(&pdev->dev, netdev);
+
+	netif_napi_add(netdev, &p->napi, octeon_mgmt_napi_poll,
+		       OCTEON_MGMT_NAPI_WEIGHT);
+
+	p->netdev = netdev;
+	p->dev = &pdev->dev;
+	p->has_rx_tstamp = false;
+
+	spin_lock_init(&p->lock);
+
+	skb_queue_head_init(&p->tx_list);
+	skb_queue_head_init(&p->rx_list);
+	tasklet_init(&p->tx_clean_tasklet,
+		     octeon_mgmt_clean_tx_tasklet, (unsigned long)p);
+
+	netdev->priv_flags |= IFF_UNICAST_FLT;
+
+	pdev->dev.coherent_dma_mask = DMA_BIT_MASK(64);
+	pdev->dev.dma_mask = &pdev->dev.coherent_dma_mask;
+
+	netif_carrier_off(netdev);
+}
+
+static int octeon_mgmt_o3_probe(struct platform_device *pdev)
+{
+	struct net_device *netdev;
+	struct octeon_mgmt *p;
+	struct mac_platform_data *pd = dev_get_platdata(&pdev->dev);
+	int result = -EINVAL;
+
+	dev_notice(&pdev->dev, "Probed %d:%d:%d\n", pd->numa_node, pd->interface, pd->port);
+	netdev = alloc_etherdev(sizeof(struct octeon_mgmt));
+	if (netdev == NULL)
+		return -ENOMEM;
+
+	octeon_mgmt_probe_common(netdev, pdev);
+
+	bgx_port_set_netdev(pdev->dev.parent, netdev);
+
+	p = netdev_priv(netdev);
+	p->has_o3_structs = true;
+	p->re_len_shift = 50;
+	p->re_code_shift = 44;
+	p->re_ts_bit = 49;
+	/* 42-bits */
+	p->re_addr_mask = 0x3ffffffffffull;
+	p->port = pd->port;
+	p->numa_node = pd->numa_node;
+	netdev->netdev_ops = &octeon_mgmt_o3_ops;
+	netdev->ethtool_ops = &octeon_mgmt_o3_ethtool_ops;
+
+	p->mix_phys = 0x1070000100000ull + 0x800 * p->port + (1ull << 36) * p->numa_node;
+	p->mix_size = 0x100;
+
+	snprintf(netdev->name, IFNAMSIZ, "mgmt%d%d", p->numa_node, p->port);
+
+	if (!devm_request_mem_region(&pdev->dev, p->mix_phys, p->mix_size,
+				     "MIX")) {
+		p->mix_phys = 0;
+		dev_err(&pdev->dev, "ERROR: request_mem_region MIX failed\n");
+		goto err;
+	}
+
+	p->mix = (u64)devm_ioremap(&pdev->dev, p->mix_phys, p->mix_size);
+
+	result = register_netdev(netdev);
+	if (result)
+		goto err;
+
+	return 0;
+err:
+	octeon_mgmt_remove(pdev);
+	return result;
+}
+
 static int octeon_mgmt_probe(struct platform_device *pdev)
 {
 	struct net_device *netdev;
@@ -1389,20 +1639,23 @@ static int octeon_mgmt_probe(struct platform_device *pdev)
 	int len;
 	int result;
 
+	/* If it isn't the old MIX, it must be the new one */
+	if (!of_device_is_compatible(pdev->dev.of_node, "cavium,octeon-5750-mix"))
+		return octeon_mgmt_o3_probe(pdev);
+
 	netdev = alloc_etherdev(sizeof(struct octeon_mgmt));
 	if (netdev == NULL)
 		return -ENOMEM;
 
-	SET_NETDEV_DEV(netdev, &pdev->dev);
+	octeon_mgmt_probe_common(netdev, pdev);
 
-	platform_set_drvdata(pdev, netdev);
 	p = netdev_priv(netdev);
-	netif_napi_add(netdev, &p->napi, octeon_mgmt_napi_poll,
-		       OCTEON_MGMT_NAPI_WEIGHT);
-
-	p->netdev = netdev;
-	p->dev = &pdev->dev;
-	p->has_rx_tstamp = false;
+	p->has_o3_structs = false;
+	p->re_len_shift = 48;
+	p->re_code_shift = 40;
+	p->re_ts_bit = 47;
+	/* 40-bits */
+	p->re_addr_mask = 0xffffffffffull;
 
 	data = of_get_property(pdev->dev.of_node, "cell-index", &len);
 	if (data && len == sizeof(*data)) {
@@ -1420,55 +1673,54 @@ static int octeon_mgmt_probe(struct platform_device *pdev)
 		goto err;
 
 	p->irq = result;
+	result = -ENXIO; /* default err from here down */
 
 	res_mix = platform_get_resource(pdev, IORESOURCE_MEM, 0);
 	if (res_mix == NULL) {
 		dev_err(&pdev->dev, "no 'reg' resource\n");
-		result = -ENXIO;
 		goto err;
 	}
 
 	res_agl = platform_get_resource(pdev, IORESOURCE_MEM, 1);
 	if (res_agl == NULL) {
 		dev_err(&pdev->dev, "no 'reg' resource\n");
-		result = -ENXIO;
 		goto err;
 	}
 
 	res_agl_prt_ctl = platform_get_resource(pdev, IORESOURCE_MEM, 3);
 	if (res_agl_prt_ctl == NULL) {
 		dev_err(&pdev->dev, "no 'reg' resource\n");
-		result = -ENXIO;
 		goto err;
 	}
 
 	p->mix_phys = res_mix->start;
 	p->mix_size = resource_size(res_mix);
-	p->agl_phys = res_agl->start;
-	p->agl_size = resource_size(res_agl);
-	p->agl_prt_ctl_phys = res_agl_prt_ctl->start;
-	p->agl_prt_ctl_size = resource_size(res_agl_prt_ctl);
-
 
 	if (!devm_request_mem_region(&pdev->dev, p->mix_phys, p->mix_size,
 				     res_mix->name)) {
+		p->mix_phys = 0;
 		dev_err(&pdev->dev, "request_mem_region (%s) failed\n",
 			res_mix->name);
-		result = -ENXIO;
 		goto err;
 	}
 
+	p->agl_phys = res_agl->start;
+	p->agl_size = resource_size(res_agl);
+
 	if (!devm_request_mem_region(&pdev->dev, p->agl_phys, p->agl_size,
 				     res_agl->name)) {
-		result = -ENXIO;
+		p->agl_phys = 0;
 		dev_err(&pdev->dev, "request_mem_region (%s) failed\n",
 			res_agl->name);
 		goto err;
 	}
 
+	p->agl_prt_ctl_phys = res_agl_prt_ctl->start;
+	p->agl_prt_ctl_size = resource_size(res_agl_prt_ctl);
+
 	if (!devm_request_mem_region(&pdev->dev, p->agl_prt_ctl_phys,
 				     p->agl_prt_ctl_size, res_agl_prt_ctl->name)) {
-		result = -ENXIO;
+		p->agl_prt_ctl_phys = 0;
 		dev_err(&pdev->dev, "request_mem_region (%s) failed\n",
 			res_agl_prt_ctl->name);
 		goto err;
@@ -1483,22 +1735,9 @@ static int octeon_mgmt_probe(struct platform_device *pdev)
 		result = -ENOMEM;
 		goto err;
 	}
-
-	spin_lock_init(&p->lock);
-
-	skb_queue_head_init(&p->tx_list);
-	skb_queue_head_init(&p->rx_list);
-	tasklet_init(&p->tx_clean_tasklet,
-		     octeon_mgmt_clean_tx_tasklet, (unsigned long)p);
-
-	netdev->priv_flags |= IFF_UNICAST_FLT;
-
 	netdev->netdev_ops = &octeon_mgmt_ops;
 	netdev->ethtool_ops = &octeon_mgmt_ethtool_ops;
 
-	netdev->min_mtu = 64 - OCTEON_MGMT_RX_HEADROOM;
-	netdev->max_mtu = 16383 - OCTEON_MGMT_RX_HEADROOM;
-
 	mac = of_get_mac_address(pdev->dev.of_node);
 
 	if (mac)
@@ -1508,11 +1747,6 @@ static int octeon_mgmt_probe(struct platform_device *pdev)
 
 	p->phy_np = of_parse_phandle(pdev->dev.of_node, "phy-handle", 0);
 
-	result = dma_coerce_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(64));
-	if (result)
-		goto err;
-
-	netif_carrier_off(netdev);
 	result = register_netdev(netdev);
 	if (result)
 		goto err;
@@ -1522,21 +1756,10 @@ static int octeon_mgmt_probe(struct platform_device *pdev)
 
 err:
 	of_node_put(p->phy_np);
-	free_netdev(netdev);
+	octeon_mgmt_remove(pdev);
 	return result;
 }
 
-static int octeon_mgmt_remove(struct platform_device *pdev)
-{
-	struct net_device *netdev = platform_get_drvdata(pdev);
-	struct octeon_mgmt *p = netdev_priv(netdev);
-
-	unregister_netdev(netdev);
-	of_node_put(p->phy_np);
-	free_netdev(netdev);
-	return 0;
-}
-
 static const struct of_device_id octeon_mgmt_match[] = {
 	{
 		.compatible = "cavium,octeon-5750-mix",
-- 
2.25.1

