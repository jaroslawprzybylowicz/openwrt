From 0511c0cfdd32a97aa1a4c1fdbbc678ad72812646 Mon Sep 17 00:00:00 2001
From: Lukasz Majczak <lma@semihalf.com>
Date: Tue, 7 May 2019 15:17:17 +0200
Subject: [PATCH 115/345] MIPS: msi-octeon: Add MSI-X support for OCTEON III.

Signed-off-by: David Daney <david.daney@cavium.com>
Signed-off-by: Chandrakala Chavva <cchavva@caviumnetworks.com>
Signed-off-by: Stijn Devriendt <sdevrien@cisco.com>
Signed-off-by: Carlos Munoz <carlos.munoz@caviumnetworks.com>
---
 arch/mips/cavium-octeon/octeon-irq.c  |  44 +-
 arch/mips/include/asm/octeon/octeon.h |  26 +-
 arch/mips/pci/msi-octeon.c            | 623 ++++++++++++++++++--------
 3 files changed, 487 insertions(+), 206 deletions(-)

diff --git a/arch/mips/cavium-octeon/octeon-irq.c b/arch/mips/cavium-octeon/octeon-irq.c
index ba69112f27db..ec8a0ea12f7e 100644
--- a/arch/mips/cavium-octeon/octeon-irq.c
+++ b/arch/mips/cavium-octeon/octeon-irq.c
@@ -67,22 +67,6 @@ struct octeon_irq_ciu_domain_data {
 
 static __read_mostly int octeon_irq_ciu_to_irq[8][64];
 
-struct octeon_ciu_chip_data {
-	union {
-		struct {		/* only used for ciu3 */
-			u64 ciu3_addr;
-			unsigned int intsn;
-		};
-		struct {		/* only used for ciu/ciu2 */
-			u8 line;
-			u8 bit;
-		};
-	};
-	int gpio_line;
-	int current_cpu;	/* Next CPU expected to take this irq */
-	int ciu_node; /* NUMA node number of the CIU */
-};
-
 struct octeon_core_chip_data {
 	struct mutex core_irq_mutex;
 	bool current_en;
@@ -115,7 +99,7 @@ static int octeon_irq_set_ciu_mapping(int irq, int line, int bit, int gpio_line,
 	return 0;
 }
 
-static void octeon_irq_free_cd(struct irq_domain *d, unsigned int irq)
+void octeon_irq_free_cd(struct irq_domain *d, unsigned int irq)
 {
 	struct irq_data *data = irq_get_irq_data(irq);
 	struct octeon_ciu_chip_data *cd = irq_data_get_irq_chip_data(data);
@@ -2286,19 +2270,17 @@ static int __init octeon_irq_init_cib(struct device_node *ciu_node,
 
 	parent_irq = irq_of_parse_and_map(ciu_node, 0);
 	if (!parent_irq) {
-		pr_err("ERROR: Couldn't acquire parent_irq for %s\n",
+		pr_err("ERROR: Couldn't acquire parent_irq for %s\n.",
 			ciu_node->name);
 		return -EINVAL;
 	}
 
 	host_data = kzalloc(sizeof(*host_data), GFP_KERNEL);
-	if (!host_data)
-		return -ENOMEM;
 	raw_spin_lock_init(&host_data->lock);
 
 	addr = of_get_address(ciu_node, 0, NULL, NULL);
 	if (!addr) {
-		pr_err("ERROR: Couldn't acquire reg(0) %s\n", ciu_node->name);
+		pr_err("ERROR: Couldn't acquire reg(0) %s\n.", ciu_node->name);
 		return -EINVAL;
 	}
 	host_data->raw_reg = (u64)phys_to_virt(
@@ -2306,7 +2288,7 @@ static int __init octeon_irq_init_cib(struct device_node *ciu_node,
 
 	addr = of_get_address(ciu_node, 1, NULL, NULL);
 	if (!addr) {
-		pr_err("ERROR: Couldn't acquire reg(1) %s\n", ciu_node->name);
+		pr_err("ERROR: Couldn't acquire reg(1) %s\n.", ciu_node->name);
 		return -EINVAL;
 	}
 	host_data->en_reg = (u64)phys_to_virt(
@@ -2314,7 +2296,7 @@ static int __init octeon_irq_init_cib(struct device_node *ciu_node,
 
 	r = of_property_read_u32(ciu_node, "cavium,max-bits", &val);
 	if (r) {
-		pr_err("ERROR: Couldn't read cavium,max-bits from %s\n",
+		pr_err("ERROR: Couldn't read cavium,max-bits from %s\n.",
 			ciu_node->name);
 		return r;
 	}
@@ -2324,7 +2306,7 @@ static int __init octeon_irq_init_cib(struct device_node *ciu_node,
 					   &octeon_irq_domain_cib_ops,
 					   host_data);
 	if (!cib_domain) {
-		pr_err("ERROR: Couldn't irq_domain_add_linear()\n");
+		pr_err("ERROR: Couldn't irq_domain_add_linear()\n.");
 		return -ENOMEM;
 	}
 
@@ -2981,6 +2963,20 @@ void octeon_fixup_irqs(void)
 
 #endif /* CONFIG_HOTPLUG_CPU */
 
+void *octeon_irq_get_ciu3_info(int node)
+{
+	return octeon_ciu3_info_per_node[node & CVMX_NODE_MASK];
+}
+
+void octeon_irq_add_block_domain(int node, uint8_t block,
+				 struct irq_domain *domain)
+{
+	struct octeon_ciu3_info *ciu3_info;
+
+	ciu3_info = octeon_ciu3_info_per_node[node & CVMX_NODE_MASK];
+	ciu3_info->domain[block] = domain;
+}
+
 struct irq_domain *octeon_irq_get_block_domain(int node, uint8_t block)
 {
 	struct octeon_ciu3_info *ciu3_info;
diff --git a/arch/mips/include/asm/octeon/octeon.h b/arch/mips/include/asm/octeon/octeon.h
index f2756ce669a6..c9a7a9844f54 100644
--- a/arch/mips/include/asm/octeon/octeon.h
+++ b/arch/mips/include/asm/octeon/octeon.h
@@ -11,6 +11,7 @@
 #include <linux/irqflags.h>
 #include <asm/octeon/cvmx.h>
 #include <asm/bitfield.h>
+#include <linux/irq.h>
 
 extern uint64_t octeon_bootmem_alloc_range_phys(uint64_t size,
 						uint64_t alignment,
@@ -280,6 +281,22 @@ union octeon_cvmemctl {
 	} s;
 };
 
+struct octeon_ciu_chip_data {
+	union {
+		struct {		/* only used for ciu3 */
+			u64 ciu3_addr;
+			unsigned int intsn;
+		};
+		struct {		/* only used for ciu/ciu2 */
+			u8 line;
+			u8 bit;
+		};
+	};
+	int gpio_line;
+	int current_cpu;	/* Next CPU expected to take this irq */
+	int ciu_node; /* NUMA node number of the CIU */
+};
+
 extern void octeon_check_cpu_bist(void);
 
 int octeon_prune_device_tree(void);
@@ -321,8 +338,15 @@ void octeon_irq_ciu3_disable(struct irq_data *data);
 void octeon_irq_ciu3_ack(struct irq_data *data);
 void octeon_irq_ciu3_mask(struct irq_data *data);
 void octeon_irq_ciu3_mask_ack(struct irq_data *data);
+int octeon_irq_ciu3_set_affinity(struct irq_data *data,
+				 const struct cpumask *dest, bool force);
+void octeon_irq_free_cd(struct irq_domain *d, unsigned int irq);
 int octeon_irq_ciu3_mapx(struct irq_domain *d, unsigned int virq,
 			 irq_hw_number_t hw, struct irq_chip *chip);
+void *octeon_irq_get_ciu3_info(int node);
+void octeon_irq_add_block_domain(int node, uint8_t block,
+				 struct irq_domain *domain);
+struct irq_domain *octeon_irq_get_block_domain(int node, uint8_t block);
 
 /* Octeon multiplier save/restore routines from octeon_switch.S */
 void octeon_mult_save(void);
@@ -381,6 +405,4 @@ extern void octeon_fixup_irqs(void);
 
 extern struct semaphore octeon_bootbus_sem;
 
-struct irq_domain *octeon_irq_get_block_domain(int node, uint8_t block);
-
 #endif /* __ASM_OCTEON_OCTEON_H */
diff --git a/arch/mips/pci/msi-octeon.c b/arch/mips/pci/msi-octeon.c
index 2a5bb849b10e..3aa03f4a83cd 100644
--- a/arch/mips/pci/msi-octeon.c
+++ b/arch/mips/pci/msi-octeon.c
@@ -3,13 +3,14 @@
  * License.  See the file "COPYING" in the main directory of this archive
  * for more details.
  *
- * Copyright (C) 2005-2009, 2010 Cavium Networks
+ * Copyright (C) 2005-2012 Cavium Inc.
  */
+#include <linux/interrupt.h>
+#include <linux/spinlock.h>
 #include <linux/kernel.h>
 #include <linux/init.h>
+#include <linux/cpu.h>
 #include <linux/msi.h>
-#include <linux/spinlock.h>
-#include <linux/interrupt.h>
 
 #include <asm/octeon/octeon.h>
 #include <asm/octeon/cvmx-npi-defs.h>
@@ -17,27 +18,25 @@
 #include <asm/octeon/cvmx-npei-defs.h>
 #include <asm/octeon/cvmx-sli-defs.h>
 #include <asm/octeon/cvmx-pexp-defs.h>
+#include <asm/octeon/cvmx-sli-defs.h>
+#include <asm/octeon/cvmx-ciu2-defs.h>
 #include <asm/octeon/pci-octeon.h>
 
-/*
- * Each bit in msi_free_irq_bitmask represents a MSI interrupt that is
- * in use.
- */
-static u64 msi_free_irq_bitmask[4];
+/* MSI major block number (8 MSBs of intsn) */
+#define MSI_BLOCK_NUMBER	0x1e
+
+#define MSI_IRQ_SIZE		256
 
 /*
- * Each bit in msi_multiple_irq_bitmask tells that the device using
- * this bit in msi_free_irq_bitmask is also using the next bit. This
- * is used so we can disable all of the MSI interrupts when a device
- * uses multiple.
+ * Each bit in msi_free_irq_bitmap represents a MSI interrupt that is
+ * in use. Each node requires its own set of bits.
  */
-static u64 msi_multiple_irq_bitmask[4];
+static DECLARE_BITMAP(msi_free_irq_bitmap[CVMX_MAX_NODES], MSI_IRQ_SIZE);
 
 /*
- * This lock controls updates to msi_free_irq_bitmask and
- * msi_multiple_irq_bitmask.
+ * This lock controls updates to msi_free_irq_bitmap.
  */
-static DEFINE_SPINLOCK(msi_free_irq_bitmask_lock);
+static DEFINE_SPINLOCK(msi_free_irq_bitmap_lock);
 
 /*
  * Number of MSI IRQs used. This variable is set up in
@@ -60,90 +59,43 @@ static int msi_irq_size;
 int arch_setup_msi_irq(struct pci_dev *dev, struct msi_desc *desc)
 {
 	struct msi_msg msg;
-	u16 control;
-	int configured_private_bits;
-	int request_private_bits;
-	int irq = 0;
-	int irq_step;
-	u64 search_mask;
-	int index;
-
-	/*
-	 * Read the MSI config to figure out how many IRQs this device
-	 * wants.  Most devices only want 1, which will give
-	 * configured_private_bits and request_private_bits equal 0.
-	 */
-	pci_read_config_word(dev, dev->msi_cap + PCI_MSI_FLAGS, &control);
+	int irq;
+	int hwirq;
+	int msi;
+	struct irq_domain *domain;
+	int node = 0; /* Must use the correct node. TODO */
 
 	/*
-	 * If the number of private bits has been configured then use
-	 * that value instead of the requested number. This gives the
-	 * driver the chance to override the number of interrupts
-	 * before calling pci_enable_msi().
+	 * We're going to search msi_free_irq_bitmap for zero bits. This
+	 * represents an MSI interrupt number that isn't in use.
 	 */
-	configured_private_bits = (control & PCI_MSI_FLAGS_QSIZE) >> 4;
-	if (configured_private_bits == 0) {
-		/* Nothing is configured, so use the hardware requested size */
-		request_private_bits = (control & PCI_MSI_FLAGS_QMASK) >> 1;
-	} else {
-		/*
-		 * Use the number of configured bits, assuming the
-		 * driver wanted to override the hardware request
-		 * value.
-		 */
-		request_private_bits = configured_private_bits;
+	spin_lock(&msi_free_irq_bitmap_lock);
+	msi = find_next_zero_bit(msi_free_irq_bitmap[node], MSI_IRQ_SIZE, 0);
+	if (msi >= MSI_IRQ_SIZE) {
+		spin_unlock(&msi_free_irq_bitmap_lock);
+		WARN(1, "arch_setup_msi_irq: Unable to find a free MSI "
+		     "interrupt");
+		return -ENOSPC;
 	}
 
-	/*
-	 * The PCI 2.3 spec mandates that there are at most 32
-	 * interrupts. If this device asks for more, only give it one.
-	 */
-	if (request_private_bits > 5)
-		request_private_bits = 0;
+	set_bit(msi, msi_free_irq_bitmap[node]);
+	spin_unlock(&msi_free_irq_bitmap_lock);
+	msg.data = msi;
 
-try_only_one:
-	/*
-	 * The IRQs have to be aligned on a power of two based on the
-	 * number being requested.
-	 */
-	irq_step = 1 << request_private_bits;
+	if (octeon_has_feature(OCTEON_FEATURE_CIU3)) {
+		/* Get the domain for the msi interrupts */
+		domain = octeon_irq_get_block_domain(node, MSI_BLOCK_NUMBER);
 
-	/* Mask with one bit for each IRQ */
-	search_mask = (1 << irq_step) - 1;
-
-	/*
-	 * We're going to search msi_free_irq_bitmask_lock for zero
-	 * bits. This represents an MSI interrupt number that isn't in
-	 * use.
-	 */
-	spin_lock(&msi_free_irq_bitmask_lock);
-	for (index = 0; index < msi_irq_size/64; index++) {
-		for (irq = 0; irq < 64; irq += irq_step) {
-			if ((msi_free_irq_bitmask[index] & (search_mask << irq)) == 0) {
-				msi_free_irq_bitmask[index] |= search_mask << irq;
-				msi_multiple_irq_bitmask[index] |= (search_mask >> 1) << irq;
-				goto msi_irq_allocated;
-			}
-		}
-	}
-msi_irq_allocated:
-	spin_unlock(&msi_free_irq_bitmask_lock);
-
-	/* Make sure the search for available interrupts didn't fail */
-	if (irq >= 64) {
-		if (request_private_bits) {
-			pr_err("arch_setup_msi_irq: Unable to find %d free interrupts, trying just one",
-			       1 << request_private_bits);
-			request_private_bits = 0;
-			goto try_only_one;
-		} else
-			panic("arch_setup_msi_irq: Unable to find a free MSI interrupt");
+		/* Get a irq for the msi intsn (hardware interrupt) */
+		hwirq = MSI_BLOCK_NUMBER << 12 | msi;
+		irq = irq_create_mapping(domain, hwirq);
+		irqd_set_trigger_type(irq_get_irq_data(irq),
+				      IRQ_TYPE_EDGE_RISING);
+	} else {
+		/* MSI interrupts start at logical IRQ OCTEON_IRQ_MSI_BIT0 */
+		irq = msi + OCTEON_IRQ_MSI_BIT0;
 	}
 
-	/* MSI interrupts start at logical IRQ OCTEON_IRQ_MSI_BIT0 */
-	irq += index*64;
-	irq += OCTEON_IRQ_MSI_BIT0;
-
 	switch (octeon_dma_bar_type) {
 	case OCTEON_DMA_BAR_TYPE_SMALL:
 		/* When not using big bar, Bar 0 is based at 128MB */
@@ -170,47 +122,12 @@ int arch_setup_msi_irq(struct pci_dev *dev, struct msi_desc *desc)
 	default:
 		panic("arch_setup_msi_irq: Invalid octeon_dma_bar_type");
 	}
-	msg.data = irq - OCTEON_IRQ_MSI_BIT0;
-
-	/* Update the number of IRQs the device has available to it */
-	control &= ~PCI_MSI_FLAGS_QSIZE;
-	control |= request_private_bits << 4;
-	pci_write_config_word(dev, dev->msi_cap + PCI_MSI_FLAGS, control);
 
 	irq_set_msi_desc(irq, desc);
 	pci_write_msi_msg(irq, &msg);
 	return 0;
 }
 
-int arch_setup_msi_irqs(struct pci_dev *dev, int nvec, int type)
-{
-	struct msi_desc *entry;
-	int ret;
-
-	/*
-	 * MSI-X is not supported.
-	 */
-	if (type == PCI_CAP_ID_MSIX)
-		return -EINVAL;
-
-	/*
-	 * If an architecture wants to support multiple MSI, it needs to
-	 * override arch_setup_msi_irqs()
-	 */
-	if (type == PCI_CAP_ID_MSI && nvec > 1)
-		return 1;
-
-	for_each_pci_msi_entry(entry, dev) {
-		ret = arch_setup_msi_irq(dev, entry);
-		if (ret < 0)
-			return ret;
-		if (ret > 0)
-			return -ENOSPC;
-	}
-
-	return 0;
-}
-
 /**
  * Called when a device no longer needs its MSI interrupts. All
  * MSI interrupts for the device are freed.
@@ -219,87 +136,158 @@ int arch_setup_msi_irqs(struct pci_dev *dev, int nvec, int type)
  */
 void arch_teardown_msi_irq(unsigned int irq)
 {
-	int number_irqs;
-	u64 bitmask;
-	int index = 0;
-	int irq0;
+	int old;
+	int node = 0; /* Must use node device is in. TODO */
 
 	if ((irq < OCTEON_IRQ_MSI_BIT0)
-		|| (irq > msi_irq_size + OCTEON_IRQ_MSI_BIT0))
-		panic("arch_teardown_msi_irq: Attempted to teardown illegal "
-		      "MSI interrupt (%d)", irq);
+		|| (irq > msi_irq_size + OCTEON_IRQ_MSI_BIT0 - 1))
+		panic("arch_teardown_msi_irq: Attempted to teardown illegal MSI interrupt (%d)",
+		      irq);
 
 	irq -= OCTEON_IRQ_MSI_BIT0;
-	index = irq / 64;
-	irq0 = irq % 64;
+	spin_lock(&msi_free_irq_bitmap_lock);
+	old = test_and_clear_bit(irq, msi_free_irq_bitmap[node]);
+	spin_unlock(&msi_free_irq_bitmap_lock);
 
-	/*
-	 * Count the number of IRQs we need to free by looking at the
-	 * msi_multiple_irq_bitmask. Each bit set means that the next
-	 * IRQ is also owned by this device.
-	 */
-	number_irqs = 0;
-	while ((irq0 + number_irqs < 64) &&
-	       (msi_multiple_irq_bitmask[index]
-		& (1ull << (irq0 + number_irqs))))
-		number_irqs++;
-	number_irqs++;
-	/* Mask with one bit for each IRQ */
-	bitmask = (1 << number_irqs) - 1;
-	/* Shift the mask to the correct bit location */
-	bitmask <<= irq0;
-	if ((msi_free_irq_bitmask[index] & bitmask) != bitmask)
-		panic("arch_teardown_msi_irq: Attempted to teardown MSI "
-		      "interrupt (%d) not in use", irq);
-
-	/* Checks are done, update the in use bitmask */
-	spin_lock(&msi_free_irq_bitmask_lock);
-	msi_free_irq_bitmask[index] &= ~bitmask;
-	msi_multiple_irq_bitmask[index] &= ~bitmask;
-	spin_unlock(&msi_free_irq_bitmask_lock);
+	if (!old) {
+		WARN(1, "arch_teardown_msi_irq: Attempted to teardown MSI "
+		     "interrupt (%d) not in use", irq);
+	}
 }
 
 static DEFINE_RAW_SPINLOCK(octeon_irq_msi_lock);
 
 static u64 msi_rcv_reg[4];
-static u64 mis_ena_reg[4];
+static u64 msi_ena_reg[4];
+
+static int (*octeon_irq_msi_to_irq)(int);
+static int (*octeon_irq_irq_to_msi)(int);
+
+static int octeon_irq_msi_to_irq_linear(int msi)
+{
+	return msi + OCTEON_IRQ_MSI_BIT0;
+}
+
+static int octeon_irq_irq_to_msi_linear(int irq)
+{
+	return irq - OCTEON_IRQ_MSI_BIT0;
+}
+
+static int octeon_irq_msi_to_irq_scatter(int msi)
+{
+	return (((msi >> 6) & 0x3) | ((msi << 2) & 0xfc)) + OCTEON_IRQ_MSI_BIT0;
+}
+
+static int octeon_irq_irq_to_msi_scatter(int irq)
+{
+	int t = irq - OCTEON_IRQ_MSI_BIT0;
+	return ((t << 6) & 0xc0) | ((t >> 2) & 0x3f);
+}
+
+#ifdef CONFIG_SMP
+
+static atomic_t affinity_in_progress[4] = {
+	ATOMIC_INIT(1),
+	ATOMIC_INIT(1),
+	ATOMIC_INIT(1),
+	ATOMIC_INIT(1)};
+
+static int octeon_irq_msi_set_affinity_pcie(struct irq_data *data,
+					    const struct cpumask *dest,
+					    bool force)
+{
+	int msi = octeon_irq_irq_to_msi(data->irq);
+	int index = msi >> 6;
+	int bit;
+	int r;
+
+	/*
+	 * If we are in the middle of updating the set, the first call
+	 * takes care of everything, do nothing successfully.
+	 */
+	if (atomic_sub_if_positive(1, affinity_in_progress + index) < 0)
+		return 0;
+
+	r = irq_set_affinity(OCTEON_IRQ_PCI_MSI0 + index, dest);
+
+	for (bit = 0; bit < 64; bit++) {
+		int partner = octeon_irq_msi_to_irq(64 * index + bit);
+		if (partner != data->irq)
+			irq_set_affinity(partner, dest);
+	}
+	atomic_add(1, affinity_in_progress + index);
+	return r;
+}
+
+static int octeon_irq_msi_set_affinity_pci(struct irq_data *data,
+					   const struct cpumask *dest,
+					   bool force)
+{
+	int msi = octeon_irq_irq_to_msi(data->irq);
+	int index = msi >> 4;
+	int bit;
+	int r;
+
+	/*
+	 * If we are in the middle of updating the set, the first call
+	 * takes care of everything, do nothing successfully.
+	 */
+	if (atomic_sub_if_positive(1, affinity_in_progress + index) < 0)
+		return 0;
+
+	r = irq_set_affinity(OCTEON_IRQ_PCI_MSI0 + index, dest);
+
+	for (bit = 0; bit < 16; bit++) {
+		int partner = octeon_irq_msi_to_irq(16 * index + bit);
+		if (partner != data->irq)
+			irq_set_affinity(partner, dest);
+	}
+	atomic_add(1, affinity_in_progress + index);
+	return r;
+}
+#endif /* CONFIG_SMP */
 
 static void octeon_irq_msi_enable_pcie(struct irq_data *data)
 {
 	u64 en;
 	unsigned long flags;
-	int msi_number = data->irq - OCTEON_IRQ_MSI_BIT0;
+	int msi_number = octeon_irq_irq_to_msi(data->irq);
 	int irq_index = msi_number >> 6;
 	int irq_bit = msi_number & 0x3f;
 
 	raw_spin_lock_irqsave(&octeon_irq_msi_lock, flags);
-	en = cvmx_read_csr(mis_ena_reg[irq_index]);
+	en = cvmx_read_csr(msi_ena_reg[irq_index]);
 	en |= 1ull << irq_bit;
-	cvmx_write_csr(mis_ena_reg[irq_index], en);
-	cvmx_read_csr(mis_ena_reg[irq_index]);
+	cvmx_write_csr(msi_ena_reg[irq_index], en);
+	cvmx_read_csr(msi_ena_reg[irq_index]);
 	raw_spin_unlock_irqrestore(&octeon_irq_msi_lock, flags);
+	unmask_msi_irq(data);
 }
 
 static void octeon_irq_msi_disable_pcie(struct irq_data *data)
 {
 	u64 en;
 	unsigned long flags;
-	int msi_number = data->irq - OCTEON_IRQ_MSI_BIT0;
+	int msi_number = octeon_irq_irq_to_msi(data->irq);
 	int irq_index = msi_number >> 6;
 	int irq_bit = msi_number & 0x3f;
 
 	raw_spin_lock_irqsave(&octeon_irq_msi_lock, flags);
-	en = cvmx_read_csr(mis_ena_reg[irq_index]);
+	en = cvmx_read_csr(msi_ena_reg[irq_index]);
 	en &= ~(1ull << irq_bit);
-	cvmx_write_csr(mis_ena_reg[irq_index], en);
-	cvmx_read_csr(mis_ena_reg[irq_index]);
+	cvmx_write_csr(msi_ena_reg[irq_index], en);
+	cvmx_read_csr(msi_ena_reg[irq_index]);
 	raw_spin_unlock_irqrestore(&octeon_irq_msi_lock, flags);
+	mask_msi_irq(data);
 }
 
 static struct irq_chip octeon_irq_chip_msi_pcie = {
 	.name = "MSI",
 	.irq_enable = octeon_irq_msi_enable_pcie,
 	.irq_disable = octeon_irq_msi_disable_pcie,
+#ifdef CONFIG_SMP
+	.irq_set_affinity = octeon_irq_msi_set_affinity_pcie,
+#endif
 };
 
 static void octeon_irq_msi_enable_pci(struct irq_data *data)
@@ -322,6 +310,9 @@ static struct irq_chip octeon_irq_chip_msi_pci = {
 	.name = "MSI",
 	.irq_enable = octeon_irq_msi_enable_pci,
 	.irq_disable = octeon_irq_msi_disable_pci,
+#ifdef CONFIG_SMP
+	.irq_set_affinity = octeon_irq_msi_set_affinity_pci,
+#endif
 };
 
 /*
@@ -339,8 +330,9 @@ static irqreturn_t __octeon_msi_do_interrupt(int index, u64 msi_bits)
 		/* Acknowledge it first. */
 		cvmx_write_csr(msi_rcv_reg[index], 1ull << bit);
 
-		irq = bit + OCTEON_IRQ_MSI_BIT0 + 64 * index;
-		do_IRQ(irq);
+		irq = octeon_irq_msi_to_irq(bit + 64 * index);
+
+		generic_handle_irq(irq);
 		return IRQ_HANDLED;
 	}
 	return IRQ_NONE;
@@ -361,35 +353,306 @@ OCTEON_MSI_INT_HANDLER_X(1);
 OCTEON_MSI_INT_HANDLER_X(2);
 OCTEON_MSI_INT_HANDLER_X(3);
 
+#if 0
+static void octeon_msi_ciu2_enable_on_cpu(unsigned int irq, int cpu)
+{
+	int core;
+	int msi = irq - OCTEON_IRQ_MSI_BIT0;
+	union cvmx_ciu2_msi_selx sel;
+
+	core = octeon_coreid_for_cpu(cpu);
+
+	sel.u64 = 0;
+	sel.s.pp_num = core;
+	sel.s.ip_num = 2; /* IP4 */
+	sel.s.en = 1;
+	cvmx_write_csr(CVMX_CIU2_MSI_SELX(msi), sel.u64);
+	/* Read back some CSR for write to complete. */
+	cvmx_read_csr(CVMX_CIU2_SUM_PPX_IP2(core));
+}
+
+static void octeon_msi_ciu2_enable(struct irq_data *data)
+{
+	int cpu;
+
+#ifdef CONFIG_SMP
+	cpu = cpumask_first(data->affinity);
+#else
+	cpu = 0;
+#endif
+	octeon_msi_ciu2_enable_on_cpu(data->irq, cpu);
+}
+
+static void octeon_msi_ciu2_disable(struct irq_data *data)
+{
+	int msi = data->irq - OCTEON_IRQ_MSI_BIT0;
+	union cvmx_ciu2_msi_selx sel;
+
+	sel.u64 = cvmx_read_csr(CVMX_CIU2_MSI_SELX(msi));
+	sel.s.en = 0;
+	cvmx_write_csr(CVMX_CIU2_MSI_SELX(msi), sel.u64);
+	cvmx_read_csr(CVMX_CIU2_INTR_CIU_READY);
+}
+
+static void octeon_msi_ciu2_ack(struct irq_data *data)
+{
+	int msi = data->irq - OCTEON_IRQ_MSI_BIT0;
+
+	cvmx_write_csr(CVMX_CIU2_MSI_RCVX(msi), 0);
+	cvmx_read_csr(CVMX_CIU2_INTR_CIU_READY);
+}
+
+#ifdef CONFIG_SMP
+static int octeon_msi_ciu2_set_affinity(struct irq_data *data,
+					const struct cpumask *dest,
+					bool force)
+{
+	int cpu = cpumask_first(dest);
+
+	/*
+	 * For CIU2-MSI, we will allow only single CPU affinity.
+	 * This .
+	 */
+	if (cpumask_weight(dest) != 1)
+		return -EINVAL;
+
+	octeon_msi_ciu2_enable_on_cpu(data->irq, cpu);
+
+	return 0;
+}
+#endif
+
+static struct irq_chip octeon_msi_chip_ciu2 = {
+	.name = "CIU2-MSI",
+	.irq_enable = octeon_msi_ciu2_enable,
+	.irq_disable = octeon_msi_ciu2_disable,
+	.irq_ack = octeon_msi_ciu2_ack,
+	.irq_mask = octeon_msi_ciu2_disable,
+	.irq_unmask = octeon_msi_ciu2_enable,
+#ifdef CONFIG_SMP
+	.irq_set_affinity = octeon_msi_ciu2_set_affinity,
+	.irq_cpu_offline = octeon_irq_cpu_offline_ciu,
+#endif
+};
+
+static void octeon_msi_ip4(void)
+{
+	union cvmx_ciu2_msired_ppx_ip4 msired;
+	int core = cvmx_get_core_num();
+
+	msired.u64 = cvmx_read_csr(CVMX_CIU2_MSIRED_PPX_IP4(core));
+
+	if (msired.s.intr)
+		do_IRQ(msired.s.msi_num + OCTEON_IRQ_MSI_BIT0);
+	else
+		spurious_interrupt();
+}
+
+static int octeon_msi_cpu_callback(struct notifier_block *nfb,
+				   unsigned long action, void *hcpu)
+{
+	unsigned int cpu = (unsigned long)hcpu;
+
+	switch (action) {
+	case CPU_DOWN_PREPARE:
+		cvmx_write_csr(CVMX_CIU2_EN_PPX_IP4_IO_W1C(cpu_logical_map(cpu)),
+			       1<<12);
+		break;
+	case CPU_ONLINE:
+	case CPU_DOWN_FAILED:
+		cvmx_write_csr(CVMX_CIU2_EN_PPX_IP4_IO_W1S(cpu_logical_map(cpu)),
+			       1<<12);
+		break;
+	default:
+		break;
+	}
+	return NOTIFY_OK;
+}
+
+static struct notifier_block octeon_msi_cpu_notifier = {
+	.notifier_call = octeon_msi_cpu_callback,
+};
+
+static int __init octeon_msi_68XX_init(void)
+{
+	int i;
+	int cpu;
+
+	/* Disable legacy handling. */
+	cvmx_write_csr(CVMX_PEXP_SLI_MSI_ENB0, 0);
+	cvmx_write_csr(CVMX_PEXP_SLI_MSI_ENB1, 0);
+	cvmx_write_csr(CVMX_PEXP_SLI_MSI_ENB2, 0);
+	cvmx_write_csr(CVMX_PEXP_SLI_MSI_ENB3, 0);
+
+	/* Disable CIU2_MSI */
+	for (i = 0; i < 256; i++)
+		cvmx_write_csr(CVMX_CIU2_MSI_SELX(i), 0);
+
+	for (i = OCTEON_IRQ_MSI_BIT0; i <= OCTEON_IRQ_MSI_LAST; i++)
+		irq_set_chip_and_handler(i, &octeon_msi_chip_ciu2, handle_edge_irq);
+
+	octeon_irq_set_ip4_handler(octeon_msi_ip4);
+	/* Enable MSIRED interrupt */
+#ifdef CONFIG_SMP
+	for_each_online_cpu(cpu)
+		cvmx_write_csr(CVMX_CIU2_EN_PPX_IP4_IO_W1S(cpu_logical_map(cpu)),
+			       1<<12);
+#else
+	cvmx_write_csr(CVMX_CIU2_EN_PPX_IP4_IO_W1S(cvmx_get_core_num()),
+			       1<<12);
+#endif
+	cvmx_read_csr(CVMX_CIU2_SUM_PPX_IP2(cvmx_get_core_num()));
+
+	register_hotcpu_notifier(&octeon_msi_cpu_notifier);
+
+	msi_irq_size = 256;
+	return 0;
+}
+#endif /* 0 */
+
+static void octeon_irq_msi_ciu3_ack(struct irq_data *data)
+{
+	u64 csr_addr;
+	struct octeon_ciu_chip_data *cd;
+	int msi;
+
+	octeon_irq_ciu3_ack(data);
+
+	cd = irq_data_get_irq_chip_data(data);
+
+	/* Acknowledge lsi (msi) interrupt (get the node from the ciu3 addr) */
+	msi = cd->intsn & 0xff;
+	csr_addr = (cd->ciu3_addr & CVMX_NODE_MASK) | msi_rcv_reg[msi >> 6];
+	cvmx_write_csr(csr_addr, 1 << (msi & 0x3f));
+}
+
+static void octeon_irq_msi_ciu3_mask_ack(struct irq_data *data)
+{
+	u64 csr_addr;
+	struct octeon_ciu_chip_data *cd;
+	int msi;
+
+	octeon_irq_ciu3_mask_ack(data);
+
+	cd = irq_data_get_irq_chip_data(data);
+
+	/* Acknowledge lsi (msi) interrupt (get the node from the ciu3 addr) */
+	msi = cd->intsn & 0xff;
+	csr_addr = (cd->ciu3_addr & CVMX_NODE_MASK) | msi_rcv_reg[msi >> 6];
+	cvmx_write_csr(csr_addr, 1 << (msi & 0x3f));
+}
+
+static struct irq_chip octeon_irq_msi_chip_ciu3 = {
+	.name = "CIU3",
+	.irq_enable = octeon_irq_ciu3_enable,
+	.irq_disable = octeon_irq_ciu3_disable,
+	.irq_ack = octeon_irq_msi_ciu3_ack,
+	.irq_mask = octeon_irq_ciu3_mask,
+	.irq_mask_ack = octeon_irq_msi_ciu3_mask_ack,
+	.irq_unmask = octeon_irq_ciu3_enable,
+#ifdef CONFIG_SMP
+	.irq_set_affinity = octeon_irq_ciu3_set_affinity,
+#endif
+};
+
+static int octeon_irq_msi_ciu3_map(struct irq_domain *d,
+				   unsigned int virq, irq_hw_number_t hw)
+{
+	return octeon_irq_ciu3_mapx(d, virq, hw, &octeon_irq_msi_chip_ciu3);
+}
+
+struct irq_domain_ops octeon_msi_domain_ciu3_ops = {
+	.map = octeon_irq_msi_ciu3_map,
+	.unmap = octeon_irq_free_cd,
+	.xlate = octeon_irq_ciu3_xlat,
+};
+
 /*
  * Initializes the MSI interrupt handling code
  */
 int __init octeon_msi_initialize(void)
 {
 	int irq;
+	struct irq_domain *domain;
 	struct irq_chip *msi;
-
-	if (octeon_dma_bar_type == OCTEON_DMA_BAR_TYPE_PCIE) {
+	u64 msi_map_reg;
+	int i;
+	int node = 0; /* Must use correct node. TODO */
+
+	/* Clear msi irq bitmap */
+	for (i = 0; i < CVMX_MAX_NODES; i++)
+		bitmap_zero(msi_free_irq_bitmap[i], MSI_IRQ_SIZE);
+
+	if (octeon_has_feature(OCTEON_FEATURE_CIU3)) {
+		/* MSI interrupts use their own domain */
+		domain = irq_domain_add_tree(NULL, &octeon_msi_domain_ciu3_ops,
+					     octeon_irq_get_ciu3_info(node));
+		octeon_irq_add_block_domain(node, MSI_BLOCK_NUMBER, domain);
+
+		/* Registers to acknowledge msi interrupts */
+		msi_rcv_reg[0] = CVMX_PEXP_SLI_MSI_RCV0;
+		msi_rcv_reg[1] = CVMX_PEXP_SLI_MSI_RCV1;
+		msi_rcv_reg[2] = CVMX_PEXP_SLI_MSI_RCV2;
+		msi_rcv_reg[3] = CVMX_PEXP_SLI_MSI_RCV3;
+		return 0;
+	}
+#if 0
+	if (OCTEON_IS_MODEL(OCTEON_CN68XX) && !OCTEON_IS_MODEL(OCTEON_CN68XX_PASS1_X))
+		return octeon_msi_68XX_init();
+#endif
+
+	if (octeon_dma_bar_type == OCTEON_DMA_BAR_TYPE_PCIE2) {
+		msi_rcv_reg[0] = CVMX_PEXP_SLI_MSI_RCV0;
+		msi_rcv_reg[1] = CVMX_PEXP_SLI_MSI_RCV1;
+		msi_rcv_reg[2] = CVMX_PEXP_SLI_MSI_RCV2;
+		msi_rcv_reg[3] = CVMX_PEXP_SLI_MSI_RCV3;
+		msi_ena_reg[0] = CVMX_PEXP_SLI_MSI_ENB0;
+		msi_ena_reg[1] = CVMX_PEXP_SLI_MSI_ENB1;
+		msi_ena_reg[2] = CVMX_PEXP_SLI_MSI_ENB2;
+		msi_ena_reg[3] = CVMX_PEXP_SLI_MSI_ENB3;
+		msi = &octeon_irq_chip_msi_pcie;
+		octeon_irq_msi_to_irq = octeon_irq_msi_to_irq_scatter;
+		octeon_irq_irq_to_msi = octeon_irq_irq_to_msi_scatter;
+		msi_map_reg = CVMX_PEXP_SLI_MSI_WR_MAP;
+	} else if (octeon_dma_bar_type == OCTEON_DMA_BAR_TYPE_PCIE) {
 		msi_rcv_reg[0] = CVMX_PEXP_NPEI_MSI_RCV0;
 		msi_rcv_reg[1] = CVMX_PEXP_NPEI_MSI_RCV1;
 		msi_rcv_reg[2] = CVMX_PEXP_NPEI_MSI_RCV2;
 		msi_rcv_reg[3] = CVMX_PEXP_NPEI_MSI_RCV3;
-		mis_ena_reg[0] = CVMX_PEXP_NPEI_MSI_ENB0;
-		mis_ena_reg[1] = CVMX_PEXP_NPEI_MSI_ENB1;
-		mis_ena_reg[2] = CVMX_PEXP_NPEI_MSI_ENB2;
-		mis_ena_reg[3] = CVMX_PEXP_NPEI_MSI_ENB3;
+		msi_ena_reg[0] = CVMX_PEXP_NPEI_MSI_ENB0;
+		msi_ena_reg[1] = CVMX_PEXP_NPEI_MSI_ENB1;
+		msi_ena_reg[2] = CVMX_PEXP_NPEI_MSI_ENB2;
+		msi_ena_reg[3] = CVMX_PEXP_NPEI_MSI_ENB3;
 		msi = &octeon_irq_chip_msi_pcie;
+		octeon_irq_msi_to_irq = octeon_irq_msi_to_irq_scatter;
+		octeon_irq_irq_to_msi = octeon_irq_irq_to_msi_scatter;
+		msi_map_reg = CVMX_PEXP_NPEI_MSI_WR_MAP;
 	} else {
 		msi_rcv_reg[0] = CVMX_NPI_NPI_MSI_RCV;
 #define INVALID_GENERATE_ADE 0x8700000000000000ULL;
 		msi_rcv_reg[1] = INVALID_GENERATE_ADE;
 		msi_rcv_reg[2] = INVALID_GENERATE_ADE;
 		msi_rcv_reg[3] = INVALID_GENERATE_ADE;
-		mis_ena_reg[0] = INVALID_GENERATE_ADE;
-		mis_ena_reg[1] = INVALID_GENERATE_ADE;
-		mis_ena_reg[2] = INVALID_GENERATE_ADE;
-		mis_ena_reg[3] = INVALID_GENERATE_ADE;
+		msi_ena_reg[0] = INVALID_GENERATE_ADE;
+		msi_ena_reg[1] = INVALID_GENERATE_ADE;
+		msi_ena_reg[2] = INVALID_GENERATE_ADE;
+		msi_ena_reg[3] = INVALID_GENERATE_ADE;
 		msi = &octeon_irq_chip_msi_pci;
+		octeon_irq_msi_to_irq = octeon_irq_msi_to_irq_linear;
+		octeon_irq_irq_to_msi = octeon_irq_irq_to_msi_linear;
+		msi_map_reg = 0;
+	}
+
+	if (msi_map_reg) {
+		int msi;
+		int ciu;
+		u64 e;
+
+		for (msi = 0; msi < 256; msi++) {
+			ciu = (msi >> 2) | ((msi << 6) & 0xc0);
+			e = (ciu << 8) | msi;
+			cvmx_write_csr(msi_map_reg, e);
+		}
 	}
 
 	for (irq = OCTEON_IRQ_MSI_BIT0; irq <= OCTEON_IRQ_MSI_LAST; irq++)
-- 
2.25.1

